---
title: "@TITLE@"
author: "@AUTHOR@"
date: "@DATE@"
output:
  html_document:
    highlight: tango
    theme: spacelab
    toc: yes
---

Welcome to the challenge webpage!

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(challenge)

data_dir = "@DATA_DIR@"
submissions_dir = "@SUBMISSIONS_DIR@"
hist_dir = "@HIST_DIR@"
email = "@EMAIL@"
date_start = "@DATE_START@"
deadline = as.POSIXct("@DEADLINE@")
baseline = "@BASELINE@"

load(file.path(data_dir, "data_train.rda"))
load(file.path(data_dir, "data_test.rda"))
load(file.path(data_dir, "y_test.rda"))
load(file.path(data_dir, "ind_quiz.rda"))

test_or_quiz <- ifelse(Sys.time()<deadline, "quiz", "test")

error_rate <- function(y_pred, y_test) {
  FP = (y_pred == "Good") & (y_test == "Bad")
  FN = (y_pred == "Bad") & (y_test == "Good")
  return(sum(FP+FN)/length(y_test))
}

average_cost <- function(y_pred, y_test) {
  FP = (y_pred == "Good") & (y_test == "Bad")
  FN = (y_pred == "Bad") & (y_test == "Good")
  return(sum(5*FP+1*FN)/length(y_test))
}

metrics = list(error = error_rate, cost = average_cost)

read_pred <- function(file, n = length(y_test)) {
  y_pred <- scan(file, what = "character")
  y_pred <- factor(y_pred, levels = c("Bad", "Good"))
  if (length(y_pred) != n)
    stop("incorrect number of predictions")
  if (any(is.na(y_pred)))
    stop("predictions contain missing values (NA)")
  return(y_pred)
}

read_err = store_new_submissions(submissions_dir, hist_dir, deadline = deadline, 
                                 valid_fun = read_pred)

history = compute_metrics(hist_dir, metrics, y_test, ind_quiz, 
                          read_fun = read_pred)

if (file.exists(file.path(hist_dir, "best.rda")))
  load(file.path(hist_dir, "best.rda"))

n_team = 0
n_submissions = 0

if (length(history)>0) {
  best_new = get_best(history, metrics=names(metrics), test_name=test_or_quiz)
 
  if (exists("best", mode = "list"))
    best_new = update_rank_diff(best_new, best)
  
  best = best_new
  
  # save best
  save(best, file = file.path(hist_dir, "best.rda"))
  
  # get stats
  n_team = sum(best[[1]]$team != baseline)
  n_submissions = sum(best[[1]]$n_submissions[best[[1]]$team != baseline])
}
```

| ![](`r glyphicon("inbox_out")`) \ **`r n_submissions`** | ![](`r glyphicon("group")`) \ **`r n_team`** | ![](`r glyphicon("calendar")`) \ **`r countdown(deadline)`** |
|:-------------:|:-------------:|:-------------:|
| submissions | teams | `r ifelse(Sys.time()<deadline, "active", "completed")` |

Last update: ```r last_update(deadline)```

# News
`r date_start`:
  ~ The challenge is open!

# Objective
**Binary classification**: predict the creditworthiness or default risk of a set of customers for the purpose of credit allowance.

To this aim, we dispose of a supervised training set: a set of customers whose answer is known. The goal is to get the highest prediction score on a test set whose answer is hidden.

# Application
1. Send an email to <`r email`> with the following information:

    - the name of the team and of the participants
    - at least one email address of a Dropbox account [![](`r glyphicon("dropbox")`)](https://www.dropbox.com/)

2. You will receive an invitation to a shared Dropbox folder named after your team.

3. Download the data.

4. Submit test prediction files in csv format in the shared Dropbox folder.

- **Note**: The number of submissions is not limited. However the scores are updated every hour.

- **submissions deadline**: `r format(deadline, "%A %d %b %Y %H:%M", usetz=TRUE)`

# Data
| Name | File | Description | Links |
| ---- | ---- | ----------- | ----- |
| Training set | `data_train.rda` | `data.frame` with ```r nrow(data_train)``` rows/customers and ```r ncol(data_train)``` columns/variables | [![](`r glyphicon("download_alt")`)](https://www.dropbox.com/***EDIT_LINK***/data_train.rda?dl=1) |
| Test set | `data_test.rda` | `data.frame` with ```r nrow(data_test)``` rows/customers and ```r ncol(data_test)``` columns/variables | [![](`r glyphicon("download_alt")`)](https://www.dropbox.com/***EDIT_LINK***/data_test.rda?dl=1) |

Load the files in R with:

```{r, eval=FALSE}
load("data_train.rda")
load("data_test.rda")
```

The variable to predict is variable `Class` which takes values `Bad` or `Good`. Training values of this variable are provided in the last column of `data_train`. `data_test` does not contain this variable as it must be predicted.

The full dataset contains 30% of `Bad` and 70% of `Good`. This proportion is kept in the training set as well as the test set.

```{r}
table(data_train$Class)/nrow(data_train)
```

For the prediction task, we have ```r ncol(data_test)``` input variables with:

- ```r sum(sapply(data_test[1,], is.numeric))``` quantitative variables of class `numeric`
- ```r sum(sapply(data_test[1,], is.factor))``` qualitative variables of class `factor`

```{r}
str(data_test)
```

Univariate statistics of these variables can easily be obtained with:
```{r, eval=FALSE}
summary(rbind(data_train[,-ncol(data_train)], data_test))
```

# Prediction
A classifier is a function that assigns `Bad` or `Good` to each sample in the test set. Such a function can be:
```{r}
predict_all_good <- function(data_test, ...) {
  y_pred = rep("Good", nrow(data_test))
  return(y_pred)
}
```
that assigns `Good` to every sample. Such a classifier does not use the training set. It corresponds to accepting every credit application.
We obtain the following result
```{r}
y_pred = predict_all_good(data_test)
```

Your goal is to program one or several classifiers using the training data to improve the perfomance of such a default decision.

# Evaluation criteria
Your prediction performance is calculated based on real answers to the test set. We use two different criteria.

### Error rate
The error rate measures the misclassification rate of your predictions, _i.e._ the number of false positives `FP` plus the number of false negatives` FN` divided by the total number. It is measured by the `error_rate` function.
```{r, echo=FALSE, collapse=TRUE, comment=NA}
dump("error_rate", "")
```

This performance metric is the 0-1 cost averaged over all predictions. The goal is to minimize the error rate. Since 70% of individuals are `Good`, the error rate associated with` predict_all_good` predictor is 0.3, while its counterpart `predict_all_bad` provides 0.7. Here `predict_all_good` is preferable.

### Average cost
However, it is considered 5 times more risky/expensive to give credit to insolvent person (false positive) than not to grant credit to a creditworthy person (false negative). The average cost is measured by the `average_cost` function.
```{r, echo=FALSE, collapse=TRUE, comment=NA}
dump("average_cost", "")
```

The goal is to minimize the average cost `predict_all_bad` here better with 0.7 to` predict_all_good` with 1.5. From the average cost point of view, it is safer not to grant credit.

# Submissions
The submissions consist in text files with the extension `.csv`, that you can export with the following command:
```{r, eval=FALSE}
write(y_pred, file = "my_pred.csv")
```

The file must contain ```r nrow(data_test)``` lines containing with one of the words `Bad` or `Good`.

All `.csv` files in your shared Dropbox folder will be automatically imported by the` read_pred` function.
```{r, echo=FALSE, collapse=TRUE, comment=NA}
dump("read_pred", "")
```

Use this function to check that your file is correctly imported.

Read errors during the import are displayed at the [Read errors] section.

Once a file is imported, the score is calculated and stored. You can delete or replace submission files, the history is kept.

# Learderboard
The ranking and scores displayed are calculated on only half of the test data. The final scores computed on the whole test set will be revealed at the end of the challenge.

Only the highest score by team among all submissions is retained.

The team ```r baseline``` corresponds to the score of the best classifier among `predict_all_good` or `predict_all_bad`, in lieu of reference to improve.

### Error rate
Last update: ```r last_update(deadline)```
```{r, echo = FALSE , results='asis'}
if (exists("best", mode = "list"))
  print_leaderboard(best, "error", test_name=test_or_quiz)
```

### Average cost
Last update: ```r last_update(deadline)```
```{r, echo = FALSE , results='asis'}
if (exists("best", mode = "list"))
  print_leaderboard(best, "cost", test_name=test_or_quiz)
```

# Submissions history

### Error rate
Last update: ```r last_update(deadline)```
```{r, echo=FALSE, fig.cap="Submissions history - Error rate", fig.height=5, fig.width=9}
# color palette
library(RColorBrewer)
palette(brewer.pal(n = max(length(history)-1, 3), name = "Dark2"))

par(mar = c(5,4,4,7) + 0.1)
plot_history(history, "error", test_name=test_or_quiz, baseline=baseline)
```

### Average cost
Last update: ```r last_update(deadline)```
```{r, echo=FALSE, fig.cap="Submissions history - Average cost", fig.height=5, fig.width=9}
par(mar = c(5,4,4,7) + 0.1)
plot_history(history, "cost", test_name=test_or_quiz, baseline=baseline)
```

### Activity
Last update: ```r last_update(deadline)```
```{r, echo=FALSE, fig.cap="Submissions history - Activity", fig.height=5, fig.width=9}
par(mar = c(5,4,4,7) + 0.1)
plot_activity(history, baseline=baseline)
```

# Read errors
Last update: ```r last_update(deadline)```
```{r, echo=FALSE}
print_readerr(read_err)
```

------------
This challenge was built using the [rchallenge](http://adrtod.github.io/rchallenge/) package.

The icons of this webpage come from [GLYPHICONS.com](http://glyphicons.com/) and are distributed under the
[CC-BY](http://creativecommons.org/licenses/by/3.0/) licence.
